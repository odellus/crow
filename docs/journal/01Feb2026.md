---
title: "TBD"
date: "2026-02-01"
---

Rabbit rabbit! I am starting my day working on my journal. I foolishly put most of my thoughts I had just now in yesterday's journal because I don't really know what all was changed.

Which means the problem is persistence. I'm supposed to be the persistent memory but too many things are happening.

Pretty sure we got streaming and some of the `tool call not found` errors fixed in the frontend but yeah. I am going to end up using a lot more of the crow_ide than I thought, so I need to figure out a way to integrate that with my plans for how to do this with vscode extensions, which seem to be the way to go because with [`code-server`](https://github.com/coder/code-server.git) I can use my browser as a testbed for my vs code extension/theme I'm working on.

So things I need to do:

1. Understand how to build a proper ACP agent with [`agent-client-protocol`](https://github.com/agentclientprotocol/python-sdk.git) by going through the tutorials by hand
2. [`OpenHands-SDK`](https://github.com/OpenHands/software-agent-sdk.git) is open source and I am working on a fork of it but I can totally try to make it either
    - completely my own and never be able to sync upstream
    - play nice and make all my changes configurable  
I lean more towards the latter
3. Because my crow_ide seems to be in a more advanced state than the [`vscode-acp`](https://github.com/omercnet/vscode-acp.git), I'm guessing I'm going to give it its own repo to live in because yeah not sure about how to do that? I want it to be a separate part of the code tbh, but yeah I guess I just need to get to work on that real quick. It's a part of the testing environment and yeah probably going to re-use a lot of the elements for my mono-extension and theme to build [trae solo](https://trae.com/solo)


---
16:30 ET

So okay what did I do today? 

1. Brought the editor/agent-client-protocol client into the crow repository so you can start with `crow editor` and you've got a little application running in the browser that we can test end to end, and I mean END-TO-END with playwright so we can verify everything is working and looks like expected
2. Went back and forth on using [our fork of `software-agent-sdk`](https://github.com/odellus/software-agent-sdk.git) or the upstream and after seeing a new error crop up I decided to go ahead and just plan to completely build a new python software agent sdk
3. Put together better testing for the acp agent in addition to the full editor for playwright as sometimes we do not need to solve problems that require the full frontend because the agent is acting up or something along those line

So I feel like I've got a very good end-to-end testing setup to start kicking off some long running scripts with openhands the way we do with [`scripts/iterative_refinement.py`](../../scripts/iterative_refinement.py)

except maybe I want to be more clever about this and figure out some way for the agents to communicate rather than just leaving each other breadcrumbs in the filesystem the way iterative refinement does.

---
17:31

been using the damn thing. need to go ahead and just bite the bullet and create a new python sdk for crow. We've got a persistent session bug/problem and now I can't send cancelation requests to the actual endpoint, which is a crucial must have for local LLMs because right now glm-4.7-flash is stuck in a loop and I need to send kill signal.


- Add filesystem support in ACP — look up in protocol
- add cancellation support to agent — all the way through to the backend because we're going to assume slow local LLM chugging away
- we're going to think about extensibility from day one and that means building the agent around 
> a __plugin system__ based on standard MCP and ACP protocols 
- we're going to use postgres for persistence of our sessions for our agents and persist **EVERYTHING** so our conversations are imminently retraceable and replayable and you can always take advantage of the load session functionality of ACP 
- we're going to go ahead and set up a full docker compose suite with our postgres and qdrant and all the other things we need in there to run crow including searxng
- we're going to create a new python virtual environment for each session with a unique identifier and location inside the project directory. or maybe each project has one. that would work too. there can be other python virtual environments and the python agent can use subprocess to interact with them, that works too, but I want an agent that doesn't think in bash but python



---
19:24

thank you but no thank you.

I want to do this right and that means building something RIGHT

I've got to figure out how to make this a ACP-first agent down to the very backbone of the architecture. 

1. fuck litellm we're just going to use openai sdk seriously — spin up a proxy in docker compose.yaml like fuck using litellm python sdk. why? use openai compatible zai and llama.cpp. we're not building this for everyone and their mama. fuck anthropic. fuck anthropic in their ear. built an extension to support anthropic. that's how we're going to do it. make it extremely extendable so you can add anthropic extension and we need to think about how we can set it up for others to do so
2. I think the tools should be MCP forward. like just use MCP for everything. if they tie into the agent state in a major way, then tie them into the agent state in a major way, but we don't need to reinvent the wheel around tool calling it's called MCP there is a standard now and it has lots of other bells and whistles we can work on too for making skills and oh yeah that's another thing
3. skills — retrieval forward. we're going to make skill generation automated along with compaction. agent is going to sit and answer 20 questions about the session before it terminates/creates new skill information/updates to log / not log and these skills are going to be tied into our agent's sdk so agent can introspect through python about their own shit/what's running them. what they see. what they think. they know where it is and it's part of their DNA and if they need to make any changes to their underlying DNA/code to get things swimming their is a whole process around that like the agent opens a PR and that's how we really pour gas on this thing and because it's all **in python** like it has to be grounded or it doesn't get through the interpreter and yeah it's also like bash++ in that you get everything you get from bash through subprocess but also like I plan to turn this thing loose developing python so I should probably just go ahead and make the environment we're working on the actual environment, unlike here where you have to go into crow because I use a mono-repo-ish thing for few shot learning but yeah just figure out some skill for the agent to use python sdk to add submodules and sync because it can just isntall package and introspect and read the help and dir and before you know it you've got a skill that's built into the system for submodule few shot directory addition for more filesystem-as-database-state fun than you can handle bro see what I mean???
4. ACP forward. everything that ACP needs we're going to implement and use a very well crafted pattern for extensions to be able to insert themselves into people's ecosystem and we'll need to be sure we nail down exactly how to do migrations with alembic and everything right? like even using sqlite we should probably do that instead of just raw sql to create the local db table lol? but yeah like I believe that we can just try to raw dog this with acp and like build our own not-react loop where if the agent terminates without a tool call we just kick it until it calls the finished tool to terminate the loop by restating the query, asking if the response as given indicates it meets the acceptance criteria of the query/task/input prompt, then that's basically the user message for max_loops until the model either says finished or runs out of iterations where it fails to call a tool

that and the session structure are basically what I need to grab. and yeah part of having package pre-installed is also having it preconfigured with the same configuration/providers as calling agent, so like I guess everything is going to be built on top of crow-ai lmfaooooo

and here's the most brilliant thing about this:

I know how to do it for the most part by hand

Like I need some help/reflection/guidance/assistance on the weird parts that are sort of new to me like alembic migrations and sqlite [which I think makes more sense and is less invasive and more transparent that setting up postgres on someone's shit. we're already going to install searxng, which btw isn't that a fucking pypi package like wtf why does it need to be in docker????? because people are going to host on internet?? that makes total sense but hold on I'm about to do an experiment


nah you can't really use pypi

hey man do you know how to add a submodule to a directory in git?

I'm trying to add a git@github.com:searxng/searxng.git to a subdirectory
